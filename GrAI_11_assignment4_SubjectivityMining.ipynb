{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Subjectivity Mining\n",
        "Matthias, Teo and Noa\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "To run this notebook, make sure that the data (olid-train-small.csv, olid-test.csv, hasoc-train.csv and hatebase_dict_vua_format.csv) are stored in a folder called **data_SM** inside 'My Drive' on google drive. Data is found [here](https://canvas.vu.nl/courses/63973/files/5284148?wrap=1).\n",
        " \n"
      ],
      "metadata": {
        "id": "ujTtZfT1HHbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiwQ5i0EfQkb"
      },
      "outputs": [],
      "source": [
        "# install necessary libraries\n",
        "!pip install simpletransformers \n",
        "!pip install pytorch-pretrained-bert pytorch-nlp\n",
        "!pip install emoji==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import LinearSVC\n",
        "from scipy.special import softmax\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "import emoji\n",
        "import regex\n",
        "from tqdm import tqdm\n",
        "from sklearn import svm\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "kMDRLy7zpDda"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive to access documents: a window should open that asks for access which you should give\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ji46opSIfe14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data (ensure that data is loaded in a non-shared folder)\n",
        "traindata_olid = pd.read_csv('/content/gdrive/MyDrive/data_SM/olid-train-small.csv')\n",
        "testdata_olid = pd.read_csv('/content/gdrive/MyDrive/data_SM/olid-test.csv')\n",
        "traindata_hasoc = pd.read_csv('/content/gdrive/MyDrive/data_SM/hasoc-train.csv')"
      ],
      "metadata": {
        "id": "1KKrwBLlgiUV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training models \n",
        "\n",
        "# bert base cased\n",
        "def train_bert(traindata, dataset_name, args):\n",
        "  \"\"\"\n",
        "  function that loads the saved trained bert-base-cased model, if file not found:\n",
        "  it starts training and saves the file in the data_SM folder\n",
        "  \n",
        "  :param dataset_name: pandas dataframe traindata\n",
        "  :param filename: name of the dataset for which the model should be saved\n",
        "  :param args: options to load bert model in the same output directory\n",
        "\n",
        "  :returns: bert model \n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # try to find the saved trained pickled model\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/bert_model_{dataset_name}.pkl', 'rb') as infile:\n",
        "      model_bert = pickle.load(infile)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    # if saved model not found: train\n",
        "    model_bert = ClassificationModel('bert', 'bert-base-cased', args=args, use_cuda=True) \n",
        "    model_bert.train_model(traindata)\n",
        "\n",
        "    # save model as pickle file\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/bert_model_{dataset_name}.pkl', 'wb') as f:\n",
        "        pickle.dump(model_bert, f)\n",
        "  \n",
        "  return model_bert\n",
        "\n",
        "\n",
        "# bertweet\n",
        "def train_bertweet(traindata, dataset_name, args):\n",
        "  \"\"\"\n",
        "  function that loads the saved trained bertweet-base model. \n",
        "  if file not found: it starts training and saves the file in the data_SM folder\n",
        "  \n",
        "  :param traindata: pandas dataframe traindata\n",
        "  :param dataset_name: name of the dataset for which the model should be saved\n",
        "  :param args: options to load bert model in the same output directory\n",
        "\n",
        "  :returns: bertweet model \n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # try to find the saved trained pickled model\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/bertweet_model_{dataset_name}.pkl', 'rb') as infile:\n",
        "      model_bertweet = pickle.load(infile)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    # if saved model not found: train\n",
        "    model_bertweet = ClassificationModel('bertweet', 'vinai/bertweet-base', args=args, use_cuda=True) \n",
        "    model_bertweet.train_model(traindata)\n",
        "\n",
        "    # save model as pickle file\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/bertweet_model_{dataset_name}.pkl', 'wb') as f:\n",
        "        pickle.dump(model_bertweet, f)\n",
        "  \n",
        "  return model_bertweet\n",
        "\n",
        "\n",
        "# hateBERT\n",
        "def train_hatebert(traindata, dataset_name, args):\n",
        "  \"\"\"\n",
        "  function that loads the saved trained hatebert-base-uncased model. \n",
        "  if file not found: it starts training and saves the file in the data_SM folder\n",
        "  \n",
        "  :param traindata: pandas dataframe traindata\n",
        "  :param dataset_name: name of the dataset for which the model should be saved\n",
        "  :param args: options to load bert model in the same output directory\n",
        "\n",
        "  :returns: bertweet model \n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # try to find the saved trained pickled model\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/hatebert_model_{dataset_name}.pkl', 'rb') as infile:\n",
        "      model_hatebert = pickle.load(infile)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    # if saved model not found: train\n",
        "    model_hatebert = ClassificationModel('bert', 'GroNLP/hateBERT', args=args, use_cuda=True) \n",
        "    model_hatebert.train_model(traindata)\n",
        "\n",
        "    # save model as pickle file\n",
        "    with open(f'/content/gdrive/MyDrive/data_SM/hatebert_model_{dataset_name}.pkl', 'wb') as f:\n",
        "        pickle.dump(model_hatebert, f)\n",
        "  \n",
        "  return model_hatebert\n"
      ],
      "metadata": {
        "id": "ZS5nb47uqFAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #### PART 2.1: In-domain experiments (olid)\n",
        "\n",
        "# # training/loading models\n",
        "\n",
        "train_args = {\"reprocess_input_data\": True,\n",
        "              \"overwrite_output_dir\": True,\n",
        "              \"use_early_stopping\": True,\n",
        "              \"early_stopping_delta\": 0.01,\n",
        "              \"early_stopping_metric\": \"mcc\",\n",
        "              \"early_stopping_metric_minimize\": False,\n",
        "              \"early_stopping_patience\": 5,\n",
        "              \"evaluate_during_training_steps\": 1000}\n",
        "\n",
        "bert_model_olid = train_bert(traindata=traindata_olid, dataset_name='olid', args=train_args)\n",
        "bertweet_model_olid = train_bertweet(traindata=traindata_olid, dataset_name='olid', args=train_args)\n",
        "hatebert_model_olid = train_hatebert(traindata=traindata_olid, dataset_name='olid', args=train_args)\n",
        "\n",
        "bert_model_hasoc = train_bert(traindata=traindata_hasoc, dataset_name='hasoc', args=train_args)\n",
        "bertweet_model_hasoc = train_bertweet(traindata=traindata_hasoc, dataset_name='hasoc', args=train_args)\n",
        "hatebert_model_hasoc = train_hatebert(traindata=traindata_hasoc, dataset_name='hasoc', args=train_args)\n"
      ],
      "metadata": {
        "id": "kDL7IG2gKDn8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_matrix(test_y, sys_y, title, labels):\n",
        "    \"\"\"\n",
        "    Function to generate a confusion matrix and returns the classification report\n",
        "\n",
        "    :param test_y: actual labels of testdata in a list\n",
        "    :param sys_y: predicted labels in a list\n",
        "    :param title: main title of the plot in string\n",
        "    :param labels: list of label names in corresponding order to be put on the axis\n",
        "    :returns: classification report in string\n",
        "    \"\"\"\n",
        "    report = classification_report(test_y, sys_y)\n",
        "    conf = confusion_matrix(test_y, sys_y)\n",
        "    ax = sns.heatmap(conf, annot=True, cmap='Blues', fmt='g', annot_kws={\"size\": 16}, cbar=False)\n",
        "    sns.set(font_scale=3)\n",
        "    ax.set_title(title,fontsize=20)\n",
        "    ax.xaxis.set_ticklabels(labels,fontsize=14)\n",
        "    ax.yaxis.set_ticklabels(labels,fontsize=14)\n",
        "    plt.ylabel('True label',fontsize=18)\n",
        "    plt.xlabel('Predicted label',fontsize=18)\n",
        "    plt_name = title.replace(' ', '_').lower()\n",
        "    plt.savefig(f\"{plt_name}.pdf\", dpi=40, bbox_inches='tight')\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "yjQ0e0UnZxhA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on the testdata\n",
        "\n",
        "def evaluate(model, testdata, title):\n",
        "  \"\"\"\n",
        "  Function to evaluate the results of a model with testdata and saves directly the confusion matrix\n",
        "\n",
        "  :param model: the trained model\n",
        "  :param testdata: testdata in pandas dataframe\n",
        "  :param title: title of model (for saving plots)\n",
        "  :returns: classification report, binary predictions and predicted probabilities\n",
        "    \"\"\"\n",
        "  \n",
        "  # results per label\n",
        "  result, model_outputs, wrong_predictions = model.eval_model(testdata, acc=sklearn.metrics.precision_recall_fscore_support)\n",
        "\n",
        "  # predictions\n",
        "  y_binary_preds = [0 if output[0] == max(output) else 1 for output in model_outputs] # binary: 1 is highest probability, 0 lowest\n",
        "  y_probs_preds = model_outputs # probabilities\n",
        "  \n",
        "  # confusion matrix & classification report\n",
        "  report = conf_matrix(test_y=testdata.labels, sys_y=y_binary_preds, title=title, labels=['NON', 'OFF'])\n",
        "  report.splitlines()\n",
        "\n",
        "  return report, y_binary_preds, y_probs_preds   "
      ],
      "metadata": {
        "id": "kN1pA9L7mlbB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HARD MAJORITY VOTE\n",
        "\n",
        "def hard_majority_vote(predictions):\n",
        "  \"\"\"\n",
        "  Function to perform the hard majority vote method based on binary predictions \n",
        "\n",
        "  :param predictions: pandas dataframe with all predicted binary values\n",
        "  :returns: list of votings\n",
        "  \"\"\"\n",
        "  \n",
        "  cols = list(predictions.columns)\n",
        "  votings = []\n",
        "  for i, row in predictions.iterrows():\n",
        "    values = [row[col] for col in cols]\n",
        "    vote = list(Counter(values).keys())[0]\n",
        "    votings.append(vote)\n",
        "  \n",
        "  return votings"
      ],
      "metadata": {
        "id": "iR8vJTSDXch5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SOFT MAJORITY VOTE\n",
        "\n",
        "def soft_majority_vote(predictions):\n",
        "  \"\"\"\n",
        "  Function to perform the hard majority vote method based on binary predictions \n",
        "\n",
        "  :param predictions: pandas dataframe with all predicted binary values\n",
        "  :returns: list of votings\n",
        "  \"\"\"\n",
        "\n",
        "  probs_per_class = defaultdict(list)\n",
        "  votings = []\n",
        "  probs = defaultdict(list)\n",
        "  for row in range(len(predictions[0])):\n",
        "\n",
        "    # non_offensive probablities\n",
        "    non_model_0 = predictions[0][row][0]\n",
        "    non_model_1 = predictions[1][row][0]\n",
        "    non_model_2 = predictions[2][row][0]\n",
        "    sum_non = non_model_0 + non_model_1 + non_model_2\n",
        "\n",
        "    probs['probs_non (bert;bertweet;hatebert)'].append(f\"{non_model_0.round(2)};{non_model_1.round(2)};{non_model_2.round(2)}\")\n",
        "\n",
        "    # offensive probabilities\n",
        "    off_model_0 = predictions[0][row][1]\n",
        "    off_model_1 = predictions[1][row][1]\n",
        "    off_model_2 = predictions[2][row][1]\n",
        "    sum_off = off_model_0 + off_model_1 + off_model_2\n",
        "\n",
        "    probs['probs_off (bert;bertweet;hatebert)'].append(f\"{off_model_0.round(2)};{off_model_1.round(2)};{off_model_2.round(2)}\")\n",
        "\n",
        "    if sum_non > sum_off:\n",
        "      votings.append(0)\n",
        "    else:\n",
        "      votings.append(1)\n",
        "    \n",
        "  return votings, probs"
      ],
      "metadata": {
        "id": "cka66uN5mK8X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(df, args):\n",
        "  \"\"\"\n",
        "  Function that adds features to the given dataframe. Features are: number of characters, \n",
        "  relative frequency of uppercase characters, number of tokens and number of special symbols, \n",
        "\n",
        "  :param df: pandas dataframe with column 'text' to determine the additional features from\n",
        "  :returns: pandas dataframe with extra feature columns \n",
        "  \"\"\"\n",
        "\n",
        "  characters, freq_upper, tokens, symbols, emojis, hate_lexs = [], [], [], [], [], []\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  if args['num_lex'] == True:\n",
        "    hate_lexicons = pd.read_csv('/content/gdrive/MyDrive/data_SM/hatebase_dict_vua_format.csv', sep=';')\n",
        "    lexicons = list(hate_lexicons['Entry'])\n",
        "\n",
        "  print(\"Adding additional features:\")\n",
        "  for text in tqdm(df.text):\n",
        "    text = str(text)\n",
        "    # number of characters\n",
        "    if args['num_chars'] == True:\n",
        "      characters.append(len(text)) \n",
        "\n",
        "    # frequence uppercase characters\n",
        "    if args['num_uppercase'] == True:\n",
        "      freq_upper.append(sum(1 for c in text if c.isupper())/len(text)) \n",
        "\n",
        "    # get number of special symbols\n",
        "    if args['num_symbols'] == True:\n",
        "      len_symbols = 0\n",
        "      for char in text:\n",
        "        if char.isalpha():\n",
        "          continue\n",
        "        elif char.isdigit():\n",
        "          continue\n",
        "        else:\n",
        "          len_symbols+=1\n",
        "      symbols.append(len_symbols)\n",
        "    \n",
        "    # get number of tokens and/or number of hatefull lexicons\n",
        "    if (args['num_tokens']==True) or (args['num_lex']==True):\n",
        "      doc = nlp(text)\n",
        "      len_tokens = 0\n",
        "      len_hate_lex = 0\n",
        "      for token in doc:\n",
        "        len_tokens += 1\n",
        "        if token.text in lexicons:\n",
        "          len_hate_lex += 1\n",
        "      tokens.append(len_tokens)\n",
        "      hate_lexs.append(len_hate_lex)\n",
        "\n",
        "    # get number of emoticons\n",
        "    if args['num_emojis'] == True:\n",
        "      num_emojis = 0\n",
        "      data = regex.findall(r'\\X', text)\n",
        "      for word in data:\n",
        "          if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "              num_emojis += 1\n",
        "      emojis.append(num_emojis)\n",
        "\n",
        "  features_cols = []\n",
        "  if args['num_chars'] == True:\n",
        "    df['num_chars'] = characters\n",
        "    features_cols.append('num_chars')\n",
        "  if args['num_uppercase'] == True:\n",
        "    df['freq_uppercase'] = freq_upper\n",
        "    features_cols.append('freq_uppercase')\n",
        "  if args['num_tokens'] == True:\n",
        "    df['num_tokens'] = tokens\n",
        "    features_cols.append('num_tokens')\n",
        "  if args['num_symbols'] == True:\n",
        "    df['num_symbols'] = symbols\n",
        "    features_cols.append('num_symbols')\n",
        "  if args['num_emojis'] == True:\n",
        "    df['num_emojis'] = emojis\n",
        "    features_cols.append('num_emojis')\n",
        "  if args['num_lex'] == True:\n",
        "    df['num_hate_lexicons'] = hate_lexs\n",
        "    features_cols.append('num_hate_lexicons')\n",
        "  \n",
        "  \n",
        "  return df, features_cols\n"
      ],
      "metadata": {
        "id": "bSPLLe6bUA1W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-FOLD + STACKED GENERALIZATION\n",
        "\n",
        "def k_fold(trainset, model_code, model_type, filename):\n",
        "  \"\"\"\n",
        "  Function to perform a five fold cross validation\n",
        "\n",
        "  :param trainset: dataset that contains training instances\n",
        "  :param model_code: the model code to be inserted for the simple transformers ClassificationModel\n",
        "  :param model_type: the model type to be inserted for the simple transformers CLassificationModel\n",
        "  :returns: pandas dataframe with output \n",
        "  \"\"\"\n",
        "  try:\n",
        "    # try to find the saved output\n",
        "    output = pd.read_excel(f'/content/gdrive/MyDrive/data_SM/{filename}.xlsx')\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    rskf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "    pred = []\n",
        "    probabilities = []\n",
        "    gold = []\n",
        "    index = []\n",
        "    for train_index, test_index in rskf.split(trainset['text'], trainset['labels']):\n",
        "      train_df = trainset.iloc[train_index]\n",
        "      test_df = trainset.iloc[test_index]\n",
        "      \n",
        "      model = ClassificationModel(model_code, model_type, args=train_args, use_cuda=True) \n",
        "      model.train_model(train_df)\n",
        "      preds, probs = model.predict(test_df.text.to_list())\n",
        "\n",
        "      gold.extend(test_df['labels'])\n",
        "      pred.extend(preds)\n",
        "      probabilities.extend(softmax(probs, axis=1)[:,1])\n",
        "      index.extend(test_index)\n",
        "\n",
        "    output = pd.DataFrame(columns = ['label', 'probabilities', 'predicted', 'id'])\n",
        "    output.label, output.probabilities, output.predicted, output.id = gold, probabilities, pred, index\n",
        "    output['id'] = [trainset.id.to_list()[idx] for idx in index]\n",
        "    output = output[['id', 'label', 'probabilities', 'predicted']]\n",
        "    output.to_excel(f'/content/gdrive/MyDrive/data_SM/{filename}.xlsx', index=False)\n",
        "  return output\n",
        "\n",
        "\n",
        "def stacking(traindata, testdata, title, feature_args):\n",
        "  \"\"\"\n",
        "  Function to perform the stacked generalization method using kfold\n",
        "\n",
        "  :param traindata: the traindata that consists of the columns id, actual labels and text\n",
        "  :param testdata: the testdata that consists of the columns id, actual labels, text, binary predictions bert, \n",
        "                   binary predictions bertweet and binary predictions hatebert\n",
        "  :param title: title to give to the matrix and results that will be saved\n",
        "  :feature_args: arguments passed that tells which additional features to include\n",
        "  :returns: binary predictions logistic regression meta learner\n",
        "  \"\"\"\n",
        "\n",
        "  # perform kfold \n",
        "  output_bert = k_fold(traindata, 'bert', 'bert-base-cased', f'kfold_bert_{title}').sort_values(by=['id']).reset_index(drop=True)\n",
        "  output_bertweet = k_fold(traindata, 'bertweet', 'vinai/bertweet-base', f'kfold_bertweet_{title}').sort_values(by=['id']).reset_index(drop=True)\n",
        "  output_berthate = k_fold(traindata, 'bert', 'GroNLP/hateBERT', f'kfold_berthate_{title}').sort_values(by=['id']).reset_index(drop=True)\n",
        "\n",
        "  # combine outputs\n",
        "  output_kfold = pd.DataFrame(columns=['pred_bert', 'pred_bertweet', 'pred_hatebert'])\n",
        "  output_kfold.pred_bert, output_kfold.pred_bertweet, output_kfold.pred_hatebert = output_bert['predicted'], output_bertweet['predicted'], output_berthate['predicted']\n",
        "\n",
        "  # add additional features\n",
        "  traindata = traindata.sort_values(by=['id']).reset_index(drop=True)\n",
        "  testdata = testdata.sort_values(by=['id']).reset_index(drop=True)\n",
        "  traindata_features, feature_cols = add_features(traindata, feature_args)\n",
        "  new_traindata = pd.concat([traindata_features, output_kfold], axis=1)\n",
        "  testdata_features, feature_cols = add_features(testdata, feature_args)\n",
        "\n",
        "  cols = ['pred_bert', 'pred_bertweet', 'pred_hatebert'] + feature_cols\n",
        "\n",
        "  # create train/test data\n",
        "  y_train = output_bert['label']\n",
        "  X_train = new_traindata.drop('labels', axis=1).drop('id', axis=1).drop('text', axis=1)\n",
        "  X_train = X_train[cols]\n",
        "  y_test = testdata_features['labels']\n",
        "  X_test = testdata_features.drop('labels', axis=1).drop('id', axis=1).drop('text', axis=1)\n",
        "  X_test = X_test[cols]\n",
        "  \n",
        "  # train logistic regressor\n",
        "  clf_lr = LogisticRegression(random_state=0, max_iter=10000).fit(X_train, y_train)\n",
        "  preds_lr = clf_lr.predict(X_test)\n",
        "\n",
        "  # show evaluations\n",
        "  report = conf_matrix(test_y=y_test, sys_y=preds_lr, title=title, labels=['NON', 'OFF'])\n",
        "  \n",
        "  return testdata_features[feature_cols], output_kfold, preds_lr, report\n"
      ],
      "metadata": {
        "id": "lGlM6-k_UuYm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bertweet olid\n",
        "bertweet_report_olid, bertweet_ybinary_olid, bertweet_yprobs_olid= evaluate(model=bertweet_model_olid, testdata=testdata_olid, title='BERTweet OLID matrix')\n",
        "print(bertweet_report_olid)"
      ],
      "metadata": {
        "id": "1_cDjlXSNRJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert olid\n",
        "bert_report_olid, bert_ybinary_olid, bert_yprobs_olid = evaluate(model=bert_model_olid, testdata=testdata_olid, title='BERT OLID matrix')\n",
        "print(bert_report_olid)"
      ],
      "metadata": {
        "id": "pIUb1D4VdMEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hatebert olid\n",
        "hatebert_report_olid, hatebert_ybinary_olid, hatebert_yprobs_olid = evaluate(model=hatebert_model_olid, testdata=testdata_olid, title='HateBERT OLID matrix')\n",
        "print(hatebert_report_olid)"
      ],
      "metadata": {
        "id": "jgAFYaKfdI73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PREDICTIONS OLID \n",
        "\n",
        "preds_olid = testdata_olid\n",
        "preds_olid['pred_bert'] = bert_ybinary_olid\n",
        "preds_olid['pred_bertweet'] = bertweet_ybinary_olid\n",
        "preds_olid['pred_hatebert'] = hatebert_ybinary_olid\n",
        "\n",
        "# hard majority voting\n",
        "preds_olid['hard_majority_vote'] = hard_majority_vote(preds_olid[['pred_bert', 'pred_bertweet', 'pred_hatebert']])\n",
        "hard_report = conf_matrix(test_y=preds_olid['labels'], sys_y=preds_olid['hard_majority_vote'], title='Hard majority vote OLID', labels=['NON', 'OFF'])\n",
        "print(hard_report)"
      ],
      "metadata": {
        "id": "0TdGbbBaZRFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# soft majority voting\n",
        "votings, probs = soft_majority_vote([bert_yprobs_olid, bertweet_yprobs_olid, hatebert_yprobs_olid])\n",
        "preds_olid = pd.concat([preds_olid, pd.DataFrame(probs)], axis=1)\n",
        "preds_olid['soft_majority_vote'] = votings\n",
        "soft_report = conf_matrix(test_y=preds_olid['labels'], sys_y=preds_olid['soft_majority_vote'], title='Soft majority vote OLID', labels=['NON', 'OFF'])\n",
        "print(soft_report)"
      ],
      "metadata": {
        "id": "rE7P5DWd_-Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stacked generalization\n",
        "\n",
        "testdata_features, kfold_preds, preds_meta, report  = stacking(traindata_olid, \n",
        "                                                               preds_olid[['id', 'labels','text', 'pred_bert', 'pred_bertweet', 'pred_hatebert']], \n",
        "                                                               title = 'Stacking OLID',\n",
        "                                                               feature_args= {'num_chars': True,\n",
        "                                                                              'num_tokens': True,\n",
        "                                                                              'num_emojis': True,\n",
        "                                                                              'num_symbols': True,\n",
        "                                                                              'num_uppercase': True,\n",
        "                                                                              'num_lex': True})\n",
        "print(report)\n",
        "preds_olid = preds_olid.sort_values(by=['id']).reset_index(drop=True)\n",
        "preds_olid['kfold_pred_bert'] = kfold_preds['pred_bert']\n",
        "preds_olid['kfold_pred_bertweet'] = kfold_preds['pred_bertweet']\n",
        "preds_olid['kfold_pred_hatebert'] = kfold_preds['pred_hatebert']\n",
        "preds_olid['stacking_preds'] = preds_meta\n",
        "preds_olid = pd.concat([preds_olid, testdata_features], axis=1)\n",
        "\n",
        "# save all results\n",
        "preds_olid.to_excel(f'/content/gdrive/MyDrive/data_SM/all_predictions_olid.xlsx',sheet_name=\"olid\",index=False)"
      ],
      "metadata": {
        "id": "p47voJF3Avv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bertweet hasoc\n",
        "bertweet_report_hasoc, bertweet_ybinary_hasoc, bertweet_yprobs_hasoc= evaluate(model=bertweet_model_hasoc, testdata=testdata_olid, title='BERTweet HASOC matrix')\n",
        "print(bertweet_report_hasoc)"
      ],
      "metadata": {
        "id": "gbWAK1S4RsxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert hasoc\n",
        "bert_report_hasoc, bert_ybinary_hasoc, bert_yprobs_hasoc= evaluate(model=bert_model_hasoc, testdata=testdata_olid, title='BERT HASOC matrix')\n",
        "print(bert_report_hasoc)"
      ],
      "metadata": {
        "id": "t6SarHL5PQmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hatebert hasoc\n",
        "hatebert_report_hasoc, hatebert_ybinary_hasoc, hatebert_yprobs_hasoc= evaluate(model=hatebert_model_hasoc, testdata=testdata_olid, title='HateBERT HASOC matrix')\n",
        "print(hatebert_report_hasoc)"
      ],
      "metadata": {
        "id": "zKqHM345z25X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PREDICTIONS HASOC \n",
        "\n",
        "preds_hasoc = testdata_olid\n",
        "preds_hasoc['pred_bert'] = bert_ybinary_hasoc\n",
        "preds_hasoc['pred_bertweet'] = bertweet_ybinary_hasoc\n",
        "preds_hasoc['pred_hatebert'] = hatebert_ybinary_hasoc\n",
        "\n",
        "# hard majority voting\n",
        "preds_hasoc['hard_majority_vote'] = hard_majority_vote(preds_hasoc[['pred_bert', 'pred_bertweet', 'pred_hatebert']])\n",
        "hard_report = conf_matrix(test_y=preds_hasoc['labels'], sys_y=preds_hasoc['hard_majority_vote'], title='Hard majority vote HASOC', labels=['NON', 'OFF'])\n",
        "print(hard_report)"
      ],
      "metadata": {
        "id": "xlrFXZmmBp86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# soft majority voting\n",
        "votings, probs = soft_majority_vote([bert_yprobs_hasoc, bertweet_yprobs_hasoc, hatebert_yprobs_hasoc])\n",
        "preds_hasoc = pd.concat([preds_hasoc, pd.DataFrame(probs)], axis=1)\n",
        "preds_hasoc['soft_majority_vote'] = votings\n",
        "soft_report = conf_matrix(test_y=preds_hasoc['labels'], sys_y=preds_hasoc['soft_majority_vote'], title='Soft majority vote HASOC', labels=['NON', 'OFF'])\n",
        "print(soft_report)"
      ],
      "metadata": {
        "id": "YyNcf5WWFGrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stacked generalization\n",
        "traindata_hasoc = pd.read_csv(f'/content/gdrive/MyDrive/data_SM/hasoc-train.csv')\n",
        "preds_hasoc = pd.read_excel(f'/content/gdrive/MyDrive/data_SM/all_predictions.xlsx', sheet_name='hasoc')\n",
        "testdata_features, kfold_preds, preds_meta, report  = stacking(traindata_hasoc, \n",
        "                                                               preds_hasoc[['id', 'labels','text', 'pred_bert', 'pred_bertweet', 'pred_hatebert']], \n",
        "                                                               title = 'Stacking HASOC',\n",
        "                                                               feature_args= {'num_chars': True,\n",
        "                                                                              'num_tokens': True,\n",
        "                                                                              'num_emojis': True,\n",
        "                                                                              'num_symbols': True,\n",
        "                                                                              'num_uppercase': True,\n",
        "                                                                              'num_lex': True})\n",
        "print(report)\n",
        "preds_hasoc = preds_hasoc.sort_values(by=['id']).reset_index(drop=True)\n",
        "preds_hasoc['kfold_pred_bert'] = kfold_preds['pred_bert']\n",
        "preds_hasoc['kfold_pred_bertweet'] = kfold_preds['pred_bertweet']\n",
        "preds_hasoc['kfold_pred_hatebert'] = kfold_preds['pred_hatebert']\n",
        "preds_hasoc['stacking_preds'] = preds_meta\n",
        "preds_hasoc = pd.concat([preds_hasoc, testdata_features], axis=1)\n",
        "\n",
        "# save all results\n",
        "preds_hasoc.to_excel(f'/content/gdrive/MyDrive/data_SM/all_predictions_hasoc.xlsx',sheet_name=\"hasoc\",index=False)"
      ],
      "metadata": {
        "id": "F0pffZT8FcsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}